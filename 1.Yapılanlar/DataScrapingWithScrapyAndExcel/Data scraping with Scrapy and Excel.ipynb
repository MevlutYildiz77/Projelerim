{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@tommynineteenies/data-scraping-with-scrapy-and-excel-57d04605d802"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import win32com.client as win32\n",
    "import os\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUSpider(scrapy.Spider):\n",
    "    name = \"GPU\"\n",
    "  \n",
    "    def start_requests(self):\n",
    "        # go to the best buy website\n",
    "        url = \"https://www.bestbuy.com/site/searchpage.jsp?st=gpu\"\n",
    "        \n",
    "        # Set the headers here because best buy disable random requests, we sending the request acting as Firefox\n",
    "        headers = {'User-agent': 'Opera/94.0.0.0 ',\"Accept-Encoding\": \"*\",    # Mozilla/5.0\n",
    "        \"Connection\": \"keep-alive\"}\n",
    "\n",
    "        # Sending the request\n",
    "        yield scrapy.http.Request(url, headers=headers)\n",
    "\n",
    "    def parse(self, response):\n",
    "        # get html element has the class of sku_item\n",
    "        for gpu in response.css('li.sku-item'):\n",
    "            yield {\n",
    "                'product_title': gpu.css('div.sku-title a::text').get(), # select div tag with css name of sku-title\n",
    "                'product_price': gpu.css('div.priceView-customer-price span::text').get(), # select div tag with css name of customer-price\n",
    "                'availability': gpu.xpath(\"//button[contains(@class, 'add-to-cart-button')]/@data-button-state\").get() # select data-button-state attibute of add-to-cart-button\n",
    "            }\n",
    "        # click on the next page button\n",
    "        next_page = response.css('a.sku-list-page-next').attrib['href']\n",
    "        # This is a recursive function to parse next page until there is no next page\n",
    "        if next_page is not None:\n",
    "            headers = {'User-agent': 'Mozilla/5.0',\"Accept-Encoding\": \"*\",\n",
    "            \"Connection\": \"keep-alive\"}\n",
    "            yield response.follow(next_page, callback=self.parse, headers = headers)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 00:04:55 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2023-02-03 00:04:55 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.2.0, Python 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 22.0.0 (OpenSSL 1.1.1o  3 May 2022), cryptography 37.0.1, Platform Windows-10-10.0.19044-SP0\n",
      "2023-02-03 00:04:55 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program started... \n"
     ]
    },
    {
     "ename": "ReactorAlreadyInstalledError",
     "evalue": "reactor already installed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mReactorAlreadyInstalledError\u001b[0m              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\TOSHIBA\\Desktop\\Projelerim\\Data scraping with Scrapy and Excel\\Data scraping with Scrapy and Excel.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TOSHIBA/Desktop/Projelerim/Data%20scraping%20with%20Scrapy%20and%20Excel/Data%20scraping%20with%20Scrapy%20and%20Excel.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# start the crawler \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TOSHIBA/Desktop/Projelerim/Data%20scraping%20with%20Scrapy%20and%20Excel/Data%20scraping%20with%20Scrapy%20and%20Excel.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m process \u001b[39m=\u001b[39m CrawlerProcess(settings \u001b[39m=\u001b[39m setting)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/TOSHIBA/Desktop/Projelerim/Data%20scraping%20with%20Scrapy%20and%20Excel/Data%20scraping%20with%20Scrapy%20and%20Excel.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m process\u001b[39m.\u001b[39;49mcrawl(GPUSpider)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TOSHIBA/Desktop/Projelerim/Data%20scraping%20with%20Scrapy%20and%20Excel/Data%20scraping%20with%20Scrapy%20and%20Excel.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m process\u001b[39m.\u001b[39mstart()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TOSHIBA/Desktop/Projelerim/Data%20scraping%20with%20Scrapy%20and%20Excel/Data%20scraping%20with%20Scrapy%20and%20Excel.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Use excel to open gpu.csv (the file we just output it )\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TOSHIBA\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\scrapy\\crawler.py:205\u001b[0m, in \u001b[0;36mCrawlerRunner.crawl\u001b[1;34m(self, crawler_or_spidercls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(crawler_or_spidercls, Spider):\n\u001b[0;32m    202\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    203\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mThe crawler_or_spidercls argument cannot be a spider object, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    204\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mit must be a spider class (or a Crawler object)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 205\u001b[0m crawler \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_crawler(crawler_or_spidercls)\n\u001b[0;32m    206\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_crawl(crawler, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\TOSHIBA\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\scrapy\\crawler.py:238\u001b[0m, in \u001b[0;36mCrawlerRunner.create_crawler\u001b[1;34m(self, crawler_or_spidercls)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(crawler_or_spidercls, Crawler):\n\u001b[0;32m    237\u001b[0m     \u001b[39mreturn\u001b[39;00m crawler_or_spidercls\n\u001b[1;32m--> 238\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_crawler(crawler_or_spidercls)\n",
      "File \u001b[1;32mc:\\Users\\TOSHIBA\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\scrapy\\crawler.py:313\u001b[0m, in \u001b[0;36mCrawlerProcess._create_crawler\u001b[1;34m(self, spidercls)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(spidercls, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    312\u001b[0m     spidercls \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspider_loader\u001b[39m.\u001b[39mload(spidercls)\n\u001b[1;32m--> 313\u001b[0m \u001b[39mreturn\u001b[39;00m Crawler(spidercls, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings, init_reactor\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\TOSHIBA\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\scrapy\\crawler.py:82\u001b[0m, in \u001b[0;36mCrawler.__init__\u001b[1;34m(self, spidercls, settings, init_reactor)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39mtwisted\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternet\u001b[39;00m \u001b[39mimport\u001b[39;00m default\n\u001b[1;32m---> 82\u001b[0m         default\u001b[39m.\u001b[39;49minstall()\n\u001b[0;32m     83\u001b[0m     log_reactor_info()\n\u001b[0;32m     84\u001b[0m \u001b[39mif\u001b[39;00m reactor_class:\n",
      "File \u001b[1;32mc:\\Users\\TOSHIBA\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\twisted\\internet\\selectreactor.py:194\u001b[0m, in \u001b[0;36minstall\u001b[1;34m()\u001b[0m\n\u001b[0;32m    191\u001b[0m reactor \u001b[39m=\u001b[39m SelectReactor()\n\u001b[0;32m    192\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtwisted\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternet\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmain\u001b[39;00m \u001b[39mimport\u001b[39;00m installReactor\n\u001b[1;32m--> 194\u001b[0m installReactor(reactor)\n",
      "File \u001b[1;32mc:\\Users\\TOSHIBA\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\twisted\\internet\\main.py:32\u001b[0m, in \u001b[0;36minstallReactor\u001b[1;34m(reactor)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtwisted\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternet\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtwisted.internet.reactor\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules:\n\u001b[1;32m---> 32\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mReactorAlreadyInstalledError(\u001b[39m\"\u001b[39m\u001b[39mreactor already installed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m twisted\u001b[39m.\u001b[39minternet\u001b[39m.\u001b[39mreactor \u001b[39m=\u001b[39m reactor\n\u001b[0;32m     34\u001b[0m sys\u001b[39m.\u001b[39mmodules[\u001b[39m\"\u001b[39m\u001b[39mtwisted.internet.reactor\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m reactor\n",
      "\u001b[1;31mReactorAlreadyInstalledError\u001b[0m: reactor already installed"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Program started... \")\n",
    "    # set the output file as csv\n",
    "    setting = {\n",
    "        \"FEEDS\": {\n",
    "            \"gpu.csv\": {\"format\": \"csv\"},\n",
    "        }\n",
    "    }\n",
    "    # start the crawler \n",
    "    process = CrawlerProcess(settings = setting)\n",
    "    process.crawl(GPUSpider)\n",
    "    process.start()\n",
    "    # Use excel to open gpu.csv (the file we just output it )\n",
    "    wb_path = \"gpu.csv\"\n",
    "    if os.path.exists(wb_path):\n",
    "        excel = win32.gencache.EnsureDispatch('Excel.Application')\n",
    "        excel.Workbooks.Open(os.path.abspath(wb_path), ReadOnly=1)\n",
    "        excel.Visible = 1\n",
    "        excel.WindowState = win32.constants.xlMaximized\n",
    "    print(\"Program finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'css'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\TOSHIBA\\Desktop\\Projelerim\\Data scraping with Scrapy and Excel\\Data scraping with Scrapy and Excel.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TOSHIBA/Desktop/Projelerim/Data%20scraping%20with%20Scrapy%20and%20Excel/Data%20scraping%20with%20Scrapy%20and%20Excel.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# select div tag with css name of sku-title\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/TOSHIBA/Desktop/Projelerim/Data%20scraping%20with%20Scrapy%20and%20Excel/Data%20scraping%20with%20Scrapy%20and%20Excel.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m product_title: \u001b[39m\"\u001b[39;49m\u001b[39mgpu\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mcss(\u001b[39m'\u001b[39m\u001b[39mdiv.sku-title a::text\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mget()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TOSHIBA/Desktop/Projelerim/Data%20scraping%20with%20Scrapy%20and%20Excel/Data%20scraping%20with%20Scrapy%20and%20Excel.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# select div tag with css name of customer-price\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TOSHIBA/Desktop/Projelerim/Data%20scraping%20with%20Scrapy%20and%20Excel/Data%20scraping%20with%20Scrapy%20and%20Excel.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m product_price: gpu\u001b[39m.\u001b[39mcss(\u001b[39m'\u001b[39m\u001b[39mdiv.priceView-customer-price span::text\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mget()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'css'"
     ]
    }
   ],
   "source": [
    "# select div tag with css name of sku-title\n",
    "product_title: gpu.css('div.sku-title a::text').get()\n",
    "# select div tag with css name of customer-price\n",
    "product_price: gpu.css('div.priceView-customer-price span::text').get()\n",
    "# select data-button-state attibute of add-to-cart-button\n",
    "availability: gpu.xpath(\"//button[contains(@class, 'add-to-cart-button')]/@data-button-state\").get()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45a60c048f4ccd6c48c495f838e9ad694b36421548fddd91d87e2ee8d4b585bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
